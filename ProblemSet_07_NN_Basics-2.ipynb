{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoKGHVI2gK_E"
   },
   "source": [
    "This notebook is part the of Dr. Christoforos Christoforou's course materials. You may not, nor may you knowingly allow others to reproduce or distribute lecture notes, course materials or any of their derivatives without the instructor's express written consent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ8TpYhQ3_V_"
   },
   "source": [
    "# Problem Set 07 - Basics of Neural Network Basics\n",
    "\n",
    "**Professor:** Dr. Christoforos Christoforou \n",
    "\n",
    "In this problem set, you will practice building a basic neural network based on the theory covered and in course and using TensorFlow, an open-source library to help you develop and train ML models. For this problem set, you will need the following library references, which are pre-installed with the colab environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TjBbvzidgTcG"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ8yWvbS5RF1"
   },
   "source": [
    "## Exercise 1: Load and proprocess the dataset.\n",
    "\n",
    "For this example, you will use the MNIST dataset, one of the benchmark image dataset. The dataset comprise images of handwritten digits. It is available in the `tensorflow.keras.dataset` module and can be loaded using the `mnist.load_data()` method. The code below showcases how to load the MINST dataset\n",
    "\n",
    "```python\n",
    "# Load the image dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "The `X_train` has dimensions (60000, 28,28) representing 60000 images, each 28 by 28 pixels. The first axis corresponds the #images and the rest of the dimensions specify the color intensity for each pixel. Similarly, `X_test` has dimension (10000, 28,28) representing 10000 observations.\n",
    "\n",
    "The `y_train` has dimensions (60000,) with values 0 to 9, representing each of the digit classes.\n",
    "\n",
    "**Task 1.1** In the cell below load the MNIST Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RZBcDWg6gn5r"
   },
   "outputs": [],
   "source": [
    "# Load the mist dataset \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPJImdNQ7B3v"
   },
   "source": [
    "Once you load the dataset, you need to convert the images into a format appropriate for the neural network to operate on them. The pre-processing steps are as follow:\n",
    "\n",
    "* Reshape data array to `channels_last` (i.e. (obs,row,cols,color)) or `channel_first` (i.e. (obs,color,row,cols)) format; depending on what format is supported by the library. Since, tensorflow expects the input vector be be given in a `channel_last` format we need to convert each image to a (obs, row, cols, color) format.\n",
    "* Convert data array to float32.\n",
    "* Normalize data array (i.e. divide by 255)\n",
    "* Convert labels to categorical variables (i.e. one-hot encoding) using the tensorflow.keras.utils.to_categorical(y_train).\n",
    "\n",
    "The code below showcases one why this can be done in python, for a `ndarray` `X_train` , and how to encode a label vector to one-hot encoding.\n",
    "\n",
    "```python\n",
    "# Get the shape of the ndarray\n",
    "(Nobs_train, n_rows, n_cols) = X_train.shape\n",
    "\n",
    "# Reshpep array to (obs,row,cols,color); convert to float and normalize\n",
    "X_train = X_train.reshape(Nobs_train,n_rows,n_cols,1).astype('float32')/255\n",
    "\n",
    "# Convert y_train labels to one-hot encoding\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train)\n",
    "```\n",
    "**Task 1.2** In the cell below, standardize the X_train, X_test, y_train and y_test, by applying the necessary preprocessing steps outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vDR9Xp-w_flG"
   },
   "outputs": [],
   "source": [
    "# Implement task 1.2 in this cell\n",
    "# Get the shape of the ndarray\n",
    "\n",
    "(Nobs_train, n_rows, n_cols) = X_train.shape\n",
    "\n",
    "# Reshpep array to (obs,row,cols,color); convert to float and normalize\n",
    "X_train = X_train.reshape(Nobs_train,n_rows,n_cols,1).astype('float32')/255\n",
    "\n",
    "# Convert y_train labels to one-hot encoding\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBfBHmUl_iU8"
   },
   "source": [
    "## Exercise 2:Building a Neural Network using high-level Keras API\n",
    "\n",
    "In this introductory example, we will build a neural network using high-level Keras API. For that, you will need several objects provided by the API; which include the `Sequential` model which is defined under the module `tensorflow.keras.models`, and two computational layers, namely the `Flatten` layer and the `Dense` layer, which are defined under the `tensorflow.keras.layers module`. These libraries have been imported at the beginning of the notebook, but to make this section self-contained, we re-import them in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L4aGWJ6yADw5"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time\n",
    "# Load the mist dataset \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St9DZ5TUAJaJ"
   },
   "source": [
    "**Defining the neural network architecture**\n",
    "We can define the neural network architecture using the keras API. First, we define a general `Sequential` model and then `add` to it the various computational layer. For our example, we first need to add a `Flatten` layer to convert the 2D image into a 1D vector, and then `add` two dense layers, one layer has 256 nodes; and the second one - which serves as an output layer- has 10 nodes. Each element in the output layer represents one of the categories we are trying to classify (i.e. digits 0 to 9) and its value correspond to the probability the input belong to each class. We can define this architecture using the `Keras API` as follows:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Flatten()) # add a layer to convert the 2D image to a 1D vector\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "Notice, the first `Dense` layer uses the `relu` activation function, and the output `Dense` layer uses the `softmax` activation function. Recall, the softmax normalizes the output vectors so that all entries are positive and sum up to one; thus the resulting output can be thought of as a probability over the categories.\n",
    "\n",
    "**Task 2.1** In the cell below, define the neural network architecture specified in the section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jDZk75uHCcVw"
   },
   "outputs": [],
   "source": [
    "# Implement task 2.1 \n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time\n",
    "# Load the mist dataset \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten()) # add a layer to convert the 2D image to a 1D vector\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQzhXhLaChGo"
   },
   "source": [
    "**Compile the Neural Network Architecture**\n",
    "Once we define the model, we need to compile it using the `model.compile` method of the API. As part of the compiling, we define can define the following parameters (among others):.\n",
    "\n",
    "* the **loss** function* to use for training the model.\n",
    "* the **optimizer** method to use in optimizing and, typically, we use the `adam` optimizer;\n",
    "* any *metrics* we want to track while fitting the model (i.e. accuracy)\n",
    "\n",
    "In this example, since we are dealing with a multi-class classification, we can set the loss function to be `categorical_crossentropy`. For your reference, the categorical cross entropy (CCE) is calculated as:\n",
    "\n",
    "$$\n",
    "CCE = -\\sum_{c\\in C} y_c log(\\sigma(\\hat{y})_c)\n",
    "$$\n",
    "\n",
    "where $y_c$ is the ground truth value of the c-th element in the output vector (i.e. c-th class), $\\hat{y}_c$ is the c-th element in the predicted output vector, and $\\sigma(.)$ is the `softmax` method. Notice, that the categorical cross entropy implementation in TensorFlow expects the label vector $y$ to be given as a `one-hot-encoding` form.\n",
    "\n",
    "The code below illustrates how to apply the `model.compile` method, to compile the network architecture\n",
    "\n",
    "```python\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "**Task 2.2:** Compile the Neural Network Architecture you defined in task 2.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tOmFLTE6E2hC"
   },
   "outputs": [],
   "source": [
    "# Use this cell to implement task 2.2 \n",
    "# Implement task 2.1 \n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time\n",
    "# Load the mist dataset \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten()) # add a layer to convert the 2D image to a 1D vector\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM_ddqbbE50P"
   },
   "source": [
    "**Training the model**\n",
    "\n",
    "Once we defined and compiled our model, we can train it by calling the `model.fit` method of the API. The `model.fit` expects as input  the training set and a number of fitting configuration parameters. Two commonly used parameters are that of `batch_size` and `epochs`. The code below illustrates how to fit a model. \n",
    "\n",
    "```python\n",
    "  # Batch size: refers to the size of data we use in a single pass; \n",
    "  batch_size = 128\n",
    "\n",
    "  # Epochs, the number of times to iterate over the dataset  \n",
    "  epochs = 15 \n",
    "\n",
    "  model.fit(X_train,y_train, batch_size=batch_size, epochs = epochs,validation_data=(X_test,y_test))\n",
    "```\n",
    "**Task 2.3** Train the model by calling the `model.fit` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XN8k3VMhF3g1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 4.0040 - accuracy: 0.8949 - val_loss: 0.9578 - val_accuracy: 0.9334\n",
      "Epoch 2/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.5886 - accuracy: 0.9446 - val_loss: 0.6541 - val_accuracy: 0.9276\n",
      "Epoch 3/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2986 - accuracy: 0.9578 - val_loss: 0.5278 - val_accuracy: 0.9458\n",
      "Epoch 4/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2089 - accuracy: 0.9664 - val_loss: 0.3925 - val_accuracy: 0.9518\n",
      "Epoch 5/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1737 - accuracy: 0.9688 - val_loss: 0.3719 - val_accuracy: 0.9549\n",
      "Epoch 6/15\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1479 - accuracy: 0.9723 - val_loss: 0.3913 - val_accuracy: 0.9514\n",
      "Epoch 7/15\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1180 - accuracy: 0.9763 - val_loss: 0.3483 - val_accuracy: 0.9596\n",
      "Epoch 8/15\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1270 - accuracy: 0.9754 - val_loss: 0.4283 - val_accuracy: 0.9527\n",
      "Epoch 9/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1462 - accuracy: 0.9734 - val_loss: 0.3332 - val_accuracy: 0.9594\n",
      "Epoch 10/15\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1280 - accuracy: 0.9760 - val_loss: 0.3058 - val_accuracy: 0.9622\n",
      "Epoch 11/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1222 - accuracy: 0.9761 - val_loss: 0.3329 - val_accuracy: 0.9601\n",
      "Epoch 12/15\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1190 - accuracy: 0.9763 - val_loss: 0.2865 - val_accuracy: 0.9589\n",
      "Epoch 13/15\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.1188 - accuracy: 0.9783 - val_loss: 0.3482 - val_accuracy: 0.9600\n",
      "Epoch 14/15\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1316 - accuracy: 0.9764 - val_loss: 0.3639 - val_accuracy: 0.9608\n",
      "Epoch 15/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9797 - val_loss: 0.3804 - val_accuracy: 0.9623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13fbdab80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this cell to implement task 2.2 \n",
    "# Implement task 2.1 \n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time\n",
    "# Load the mist dataset \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten()) # add a layer to convert the 2D image to a 1D vector\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics='accuracy')\n",
    "\n",
    "# Use this cell in implement task 2.3 \n",
    "# Batch size: refers to the size of data we use in a single pass; \n",
    "batch_size = 128\n",
    "\n",
    "  # Epochs, the number of times to iterate over the dataset  \n",
    "epochs = 15 \n",
    "\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs = epochs,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wscDSydsGWvq"
   },
   "source": [
    "**Evaluating the Network model performance**\n",
    "\n",
    "Once the model has been trained, we can evaluate its performance on a test set by using `model.evaluate` method. The `evaluate` method task as input a validation set and returns a tuple which includes the loss score and an accuracy score. The code below illustrates how to use the evaluate method\n",
    "\n",
    "```python\n",
    "(loss_score, accuracy_score) = model.evaluate(X_test,y_test,verbose=0)\n",
    "```\n",
    "\n",
    "**Task 2.4:** Evaluate the model you trained in task 2.3 and report its accuracy and loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "-_hMj5oVHavx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.7040 - accuracy: 0.1468 - val_loss: 12.1498 - val_accuracy: 0.2457\n",
      "Epoch 2/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.1410 - accuracy: 0.1797 - val_loss: 11.6810 - val_accuracy: 0.2751\n",
      "Epoch 3/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 7.0358 - accuracy: 0.1839 - val_loss: 12.1519 - val_accuracy: 0.2458\n",
      "Epoch 4/15\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 6.9700 - accuracy: 0.1886 - val_loss: 11.6112 - val_accuracy: 0.2796\n",
      "Epoch 5/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 6.9518 - accuracy: 0.1921 - val_loss: 11.6670 - val_accuracy: 0.2760\n",
      "Epoch 6/15\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 6.9411 - accuracy: 0.1902 - val_loss: 11.4801 - val_accuracy: 0.2877\n",
      "Epoch 7/15\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 6.8568 - accuracy: 0.1945 - val_loss: 11.4722 - val_accuracy: 0.2879\n",
      "Epoch 8/15\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 6.9965 - accuracy: 0.1859 - val_loss: 11.5019 - val_accuracy: 0.2864\n",
      "Epoch 9/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 6.9213 - accuracy: 0.1885 - val_loss: 11.5486 - val_accuracy: 0.2835\n",
      "Epoch 10/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 6.9220 - accuracy: 0.1927 - val_loss: 11.6593 - val_accuracy: 0.2766\n",
      "Epoch 11/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 6.8960 - accuracy: 0.1908 - val_loss: 11.7485 - val_accuracy: 0.2711\n",
      "Epoch 12/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.2194 - accuracy: 0.1722 - val_loss: 12.6254 - val_accuracy: 0.2165\n",
      "Epoch 13/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 7.4961 - accuracy: 0.1551 - val_loss: 12.8832 - val_accuracy: 0.2007\n",
      "Epoch 14/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 7.4536 - accuracy: 0.1576 - val_loss: 12.6986 - val_accuracy: 0.2121\n",
      "Epoch 15/15\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 7.4832 - accuracy: 0.1568 - val_loss: 12.7527 - val_accuracy: 0.2087\n",
      "Loss  [12.752694129943848, 0.2087000012397766]\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to implement task 2.2 \n",
    "# Implement task 2.1 \n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time\n",
    "# Load the mist dataset \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten()) # add a layer to convert the 2D image to a 1D vector\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics='accuracy')\n",
    "\n",
    "# Use this cell in implement task 2.3 \n",
    "# Batch size: refers to the size of data we use in a single pass; \n",
    "batch_size = 128\n",
    "\n",
    "  # Epochs, the number of times to iterate over the dataset  \n",
    "epochs = 15 \n",
    "\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs = epochs,validation_data=(X_test,y_test))\n",
    "\n",
    "# Use this cell to implement task 2.4\n",
    "loss = (loss_score, accuracy_score) = model.evaluate(X_test,y_test,verbose=0)\n",
    "\n",
    "print(\"Loss \",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLlxlepHHeek"
   },
   "source": [
    "**Apply the trained model**\n",
    "\n",
    "Once the model is trained, and the evaluation shows an acceptable performance, we can apply the model to make prediction on new unseen data using the `model.predict` method. The `model.predict(X_test)` returns a ndarry of shape (obs, 10) which represents a probability vector over categories. To make a concrete class assignment we can return the class with the hieghest predicted probability. We can identify the entry with the highest probability for each vector by using the `np.argmax` method accross each row (i.e. accross axis 1). The code above illustrates how to do that.\n",
    "\n",
    "```python\n",
    "y_predict = np.argmax(model.predict(X_test),axis=1)\n",
    "```\n",
    "\n",
    "**Task 2.5** Apply the model you trained in task 2.4 on the test set and report its performance. Moreover, identify the first three instances the model misclassifies and display their image; indicating in the title the predicted value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "P3kDTyA2J9w1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to implement task 2.5 \n",
    "y_predict = np.argmax(model.predict(X_test),axis=1)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwVSJlyQKBN-"
   },
   "source": [
    "## Exercise 3\n",
    "In the first to exercises, you reproduced a network architecture provided to you. In this exercise you are expected to create your own architecture to outperform the model from exercise 2. Repeat all the steps including, defining the network architecture; compile your model, train your model and evaluate your model. You are a allowed to use the `Flatten` layer, and as many `Dense` layers as you like (optionally, we might consider using a `Dropout` layer - which we did not discuss yet). Some things you can try to improve your architecture are\n",
    "* Introduce mode Dense layers.\n",
    "* Change the number of nodes in each layer.\n",
    "* Introduce one or more Dropout layers [see an example tutorial](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "7TlzRjKWLwOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"flatten_37_input:0\", shape=(None, 28, 28), dtype=uint8), but it was called on an input with incompatible shape (None, 784).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"flatten_37_input:0\", shape=(None, 28, 28), dtype=uint8), but it was called on an input with incompatible shape (None, 784).\n",
      "911/922 [============================>.] - ETA: 0s - loss: 2.4873 - accuracy: 0.0998WARNING:tensorflow:Model was constructed with shape (None, 28, 28) for input Tensor(\"flatten_37_input:0\", shape=(None, 28, 28), dtype=uint8), but it was called on an input with incompatible shape (None, 784).\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4869 - accuracy: 0.0998 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 2/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4895 - accuracy: 0.1000 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 3/15\n",
      "922/922 [==============================] - 2s 3ms/step - loss: 2.4866 - accuracy: 0.1002 - val_loss: 14.2117 - val_accuracy: 0.1180\n",
      "Epoch 4/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4815 - accuracy: 0.1004 - val_loss: 14.2045 - val_accuracy: 0.1180\n",
      "Epoch 5/15\n",
      "922/922 [==============================] - 2s 3ms/step - loss: 2.4841 - accuracy: 0.1004 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 6/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4923 - accuracy: 0.0999 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 7/15\n",
      "922/922 [==============================] - 2s 3ms/step - loss: 2.4941 - accuracy: 0.0997 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 8/15\n",
      "922/922 [==============================] - 2s 3ms/step - loss: 2.4838 - accuracy: 0.0998 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 9/15\n",
      "922/922 [==============================] - 4s 5ms/step - loss: 2.4792 - accuracy: 0.0999 - val_loss: 14.2162 - val_accuracy: 0.1180\n",
      "Epoch 10/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4907 - accuracy: 0.1002 - val_loss: 14.2136 - val_accuracy: 0.1180\n",
      "Epoch 11/15\n",
      "922/922 [==============================] - 2s 3ms/step - loss: 2.4811 - accuracy: 0.1008 - val_loss: 14.2044 - val_accuracy: 0.1180\n",
      "Epoch 12/15\n",
      "922/922 [==============================] - 2s 3ms/step - loss: 2.4894 - accuracy: 0.1005 - val_loss: 14.1696 - val_accuracy: 0.1190\n",
      "Epoch 13/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4771 - accuracy: 0.1002 - val_loss: 14.1839 - val_accuracy: 0.1200\n",
      "Epoch 14/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4826 - accuracy: 0.1000 - val_loss: 14.1839 - val_accuracy: 0.1200\n",
      "Epoch 15/15\n",
      "922/922 [==============================] - 3s 3ms/step - loss: 2.4907 - accuracy: 0.1001 - val_loss: 14.1839 - val_accuracy: 0.1200\n",
      "Evaluate\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 14.0709 - accuracy: 0.1262\n",
      "loss - acc: [14.070941925048828, 0.12620000541210175]\n",
      "shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to implement task 2.2 \n",
    "# Implement task 2.1 \n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(196,), name=\"digits\")\n",
    "x = layers.Flatten()\n",
    "x = layers.Dense(16, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(16, activation=\"relu\", name=\"dense_2\")(x)\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "x_val = x_train[-1000:]\n",
    "y_val = y_train[-1000:]\n",
    "x_train = x_train[:-1000]\n",
    "y_train = y_train[:-1000]\n",
    "\n",
    "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics='accuracy')\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=15, validation_data=(x_val, y_val))\n",
    "\n",
    "print(\"Evaluate\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"loss - acc:\", results)\n",
    "print(\"shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQacnVuoMA2E"
   },
   "source": [
    "## Compare the models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKLn6IHSLvyx"
   },
   "outputs": [],
   "source": [
    "# Use this cell to report the performance of your network, and compare it to the network in exercise 2. \n",
    "Exercise 2\n",
    "accuracy: 0.2087\n",
    "Loss  [12.752694129943848, 0.2087000012397766]\n",
    "\n",
    "Exercise 3\n",
    "accuracy: 0.1262\n",
    "Loss  [14.070941925048828, 0.12620000541210175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80gXdanCgZ2v"
   },
   "source": [
    "Copyright Statement: Copyright © 2020 Christoforou. The materials provided by the instructor of this course, including this notebook, are for the use of the students enrolled in the course. Materials are presented in an educational context for personal use and study and should not be shared, distributed, disseminated or sold in print — or digitally — outside the course without permission. You may not, nor may you knowingly allow others to reproduce or distribute lecture notes, course materials as well as any of their derivatives without the instructor's express written consent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ProblemSet_07_NN_Basics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
